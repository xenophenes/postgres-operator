[
{
	"uri": "/how-it-works/",
	"title": "How it Works",
	"tags": [],
	"description": "",
	"content": "Table of Contents Reference Architecture Custom Resource Definitions Command Line Interface Operator Deployment CLI Design Verbs   Affinity Debugging Persistent Volumes PostgreSQL Operator Deployment Strategies Strategies Specifying a Strategy Strategy Template Files Default Cluster Deployment Strategy (1) Cluster Deletion Custom Postgres Configurations Metrics Collection     v2.6, 2018-04-25\n Reference Architecture So, what does the Postgres Operator actually deploy when you create a cluster?\n   On this diagram, objects with dashed lines are things that are optionally deployed as part of a Postgres Cluster by the Operator and objects with solid lines are the fundamental and required components.\n For example, within the Primary Deployment, the metrics container is completely optional, you can deploy that component using Operator configuration or command line arguments if you want to cause metrics to be collected from the Postgres container.\n Replica deployments are just like the Primary deployment but are optional. You do not have to create a Replica at all unless you need that capability. As you scale up your Postgres cluster, the standard set of components gets deployed and replication to the Primary is started.\n Lastly, in a future release, you will be able to optionally deploy a Pgpool service and Deployment that can act as a SQL router to your Postgres cluster.\n Notice that each cluster deployment gets its own unique Persistent Volumes. Each volume can use different storage configurations which is quite powerful.\n   Custom Resource Definitions Kubernetes Custom Resource Definitions are used in the design of the postgres Operator to define:\n   Cluster - pgclusters\n  Backup - pgbackups\n  Upgrade - pgupgrades\n  Policy - pgpolicies\n  Tasks - pgtasks\n   A PostgreSQL Cluster is made up of multiple Deployments and Services. Optionally you can add a metrics collection container to your database pods.\n   Command Line Interface The pgo command line interface (CLI) is used by a normal end-user to create databases or clusters, or make changes to existing databases.\n The CLI interacts interacts with the apiserver REST API deployed within the postgres-operator Deployment.\n From the CLI, users can view existing clusters that were deployed using the CLI and Operator. Objects that were not created by the Crunchy Operator are now viewable from the CLI.\n   Operator Deployment The postgres Operator runs within a Deployment in the Kubernetes cluster. An administrator will deploy the postgres Operator Deployment using the provided script. Once installed and running, the Operator pod will start watching for certain defined events.\n The operator watches for create/update/delete actions on the pgcluster custom resource definitions. When the CLI creates for example a new pgcluster custom resource definition, the operator catches that event and creates pods and services for that new cluster request.\n   CLI Design The CLI uses the cobra package to implement CLI functionality like help text, config file processing, and command line parsing.\n The pgo client is essentially a REST client which communicates to the pgo-apiserver REST server running within the Operator pod. In some cases you might want to split the apiserver out into its own Deployment but the default deployment has a consolidated pod that contains both the apiserver and operator containers simply for convenience of deployment and updates.\n Verbs A user works with the CLI by entering verbs to indicate what they want to do, as follows:\n pgo show cluster all pgo delete cluster db1 db2 db3 pgo create cluster mycluster   In the above example, the show, backup, delete, and create verbs are used. The CLI is case sensitive and supports only lowercase.\n    Affinity You can have the Operator add an affinity section to a new Cluster Deployment if you want to cause Kube to attempt to schedule a Primary cluster to a specific Kube node.\n You can see the nodes on your Kube cluster by:\n kubectl get nodes   You can then specify one of those names (e.g. kubeadm-node2) when creating a cluster:\n pgo create cluster thatcluster --node-name=kubeadm-node2   The affinity rule inserted in the Deployment will used a preferred strategy so that if the node were down or not available, Kube would go ahead and schedule the Pod on another node.\n You can always view the actual node your cluster pod is scheduled on by:\n kubectl get pod -o wide   When you scale up a Cluster and add a replica, the scaling will take into account the use of \"--node-name\". If it sees that a cluster was created with a specific node name, then the replica Deployment will add an affinity rule to attempt to schedule the replica on a different node than the node the primary is schedule on. This gets you a simple for of High Availability so that your primary and replicas will not live on the same Kube node.\n   Debugging To see if the operator pod is running enter the following:\n kubectl get pod -l 'name=postgres-operator'   To verify the operator is running and has deployed the Custom Resources execute the following:\n kubectl get crd NAME KIND pgbackups.cr.client-go.k8s.io CustomResourceDefinition.v1beta1.apiextensions.k8s.io pgclusters.cr.client-go.k8s.io CustomResourceDefinition.v1beta1.apiextensions.k8s.io pgpolicies.cr.client-go.k8s.io CustomResourceDefinition.v1beta1.apiextensions.k8s.io pgpolicylogs.cr.client-go.k8s.io CustomResourceDefinition.v1beta1.apiextensions.k8s.io pgupgrades.cr.client-go.k8s.io CustomResourceDefinition.v1beta1.apiextensions.k8s.io pgtasks.cr.client-go.k8s.io CustomResourceDefinition.v1beta1.apiextensions.k8s.io     Persistent Volumes Currently the operator does not delete persistent volumes by default, it will delete the claims on the volumes. Starting with release 2.4, the Operator will create Jobs that actually run rm commands on the data volumes before actually removing the Persistent Volumes if the user passes a --delete-data flag when deleting a database cluster.\n Likewise, if the user passes --delete-backups during cluster deletion a Job is created to remove all the backups for a cluster include the related Persistent Volume.\n   PostgreSQL Operator Deployment Strategies This section describes the various deployment strategies offered by the operator. A deployment in this case is the set of objects created in Kubernetes when a custom resource definition of type pgcluster is created. CRDs are created by the pgo client command and acted upon by the postgres operator.\n Strategies To support different types of deployments, the operator supports multiple strategy implementations. Currently there is only a default cluster strategy.\n In the future, more deployment strategies will be supported to offer users more customization to what they see deployed in their Kube cluster.\n Being open source, users can also write their own strategy!\n  Specifying a Strategy In the pgo client configuration file, there is a CLUSTER.STRATEGY setting. The current value of the default strategy is 1. If you don\u0026#8217;t set that value, the default strategy is assumed. If you set that value to something not supported, the operator will log an error.\n  Strategy Template Files Each strategy supplies its set of templates used by the operator to create new pods, services, etc.\n When the operator is deployed, part of the deployment process is to copy the required strategy templates into a ConfigMap (operator-conf) that gets mounted into /operator-conf within the operator pod.\n The directory structure of the strategy templates is as follows:\n |-- backup-job.json |-- cluster | |-- 1 | |-- cluster-deployment-1.json | |-- cluster-replica-deployment-1.json | |-- cluster-service-1.json | |-- pvc.json   In this structure, each strategy\u0026#8217;s templates live in a subdirectory that matches the strategy identifier. The default strategy templates are denoted by the value of 1 in the directory structure above.\n If you add another strategy, the file names must be unique within the entire strategy directory. This is due to the way the templates are stored within the ConfigMap.\n  Default Cluster Deployment Strategy (1) Using the default cluster strategy, a cluster when created by the operator will create the following on a Kube cluster:\n   deployment running a Postgres primary container with replica count of 1\n  service mapped to the primary Postgres database\n  service mapped to the replica Postgres database\n  PVC for the primary will be created if not specified in configuration, this assumes you are using a non-shared volume technology (e.g. Amazon EBS), if the CLUSTER.PVC_NAME value is set in your configuration then a shared volume technology is assumed (e.g. HostPath or NFS), if a PVC is created for the primary, the naming convention is clustername-pvc where clustername is the name of your cluster.\n   If you want to add a Postgres replica to a cluster, you will scale the cluster, for each replica-count, a Deployment will be created that acts as a Postgres replica.\n This is very different than using say a StatefulSet to scale up Postgres. Why would I do it this way? Imagine a case where you want different parts of your Postgres cluster to use different storage configurations, I can do that by doing specific placement and deployments of each part of the cluster.\n This same concept applies to node selection for your Postgres cluster components. The Operator will let you define precisely which node you want each Postgres component to be placed upon using node affinity rules.\n  Cluster Deletion When you run the following:\n pgo delete cluster mycluster   The cluster and its services will be deleted. However the data files and backup files will remain, same with the PVCs for this cluster, they all remain.\n However, to remove the data files from the PVC you can pass a flag:\n --delete-data   which will cause a workflow to be started to actually remove the data files on the primary cluster deployment PVC.\n Also, if you pass a flag:\n --delete-backups   it will cause all the backup files to be removed.\n The data removal workflow includes the following steps:\n   create a pgtask CRD to hold the PVC name and cluster name to be removed\n  the CRD is watched, and on an ADD will cause a Job to be created that will run the rmdata container using the PVC name and cluster name as parameters which determine the PVC to mount, and the file path to remove under that PVC\n  the rmdata Job is watched by the Operator, and upon a successful status completion the actual PVC is removed\n   This workflow insures that a PVC is not removed until all the data files are removed. Also, a Job was used for the removal of files since that can be a time consuming task.\n The files are removed by the rmdata container which essentially issues the following command to remove the files:\n rm -rf /pgdata/\u0026lt;some path\u0026gt;    Custom Postgres Configurations Starting in release 2.5, users and administrators can specify a custom set of Postgres configuration files be used when creating a new Postgres cluster. The configuration files you can change include:\n   postgresql.conf\n  pg_hba.conf\n  setup.sql\n   Different configurations for Postgres might be defined for the following:\n   OLTP types of databases\n  OLAP types of databases\n  High Memory\n  Minimal Configuration for Development\n  Project Specific configurations\n  Special Security Requirements\n   Global ConfigMap If you create a configMap called pgo-custom-pg-config with any of the above files within it, new clusters will use those configuration files when setting up a new database instance. You do NOT have to specify all of the configuration files, its up to your use case which ones to use.\n An example set of configuration files and a script to create the global configMap is found at:\n $COROOT/examples/custom-config   If you run the create.sh script there, it will create the configMap that will include the Postgres configuration files within that directory.\n  Config Files Purpose The postgresql.conf file is the main Postgresql configuration file allowing you to define a wide variety of tuning parameters and features.\n The pg_hba.conf file is the way Postgresql secures down client access.\n The setup.sql file is a Crunchy Container Suite configuration file used to initially populate the database after the initial initdb is run when the database is first created. You would make changes to this if you wanted to define what database objects always are created.\n  Granular Config Maps So, lets say you want to use a different set of configuration files for different clusters instead of having just a single configuration (e.g. Global Config Map). You can create your own set of ConfigMaps with their own set of Postgresql configuration files. When creating new clusters, you can pass a --custom-config flag along with the name of your ConfigMap and that will be used for that specific cluster or set of clusters.\n  Default Lets say you are happy with the default Postgresql configuration files that ship with the Crunchy Postgres container. You don\u0026#8217;t have to do anything essentially, just keep using the Operator as normal. Just be sure to not define a global configMap or pass the command line flag.\n  Labeling You will notice that when a custom configMap is used in cluster creation, the Operator labels the primary Postgres Deployment with a label that hase a custom-config label and a value of what configMap was used when creating the database.\n Commands coming in future releases will take advantage of this labeling.\n   Metrics Collection If you add a --metrics flag to pgo create cluster it will cause the crunchy-collect container to be added to your Postgres cluster.\n That container requires you run the crunchy-metrics containers as defined within the crunchy-containers project.\n The prometheus push gateway that is deployed as part of the crunchy-metrics example is a current requirement for the metrics solution. This will change in an upcoming release of the crunchy-containers project and there will no longer be a requirement for the push gateway to be deployed.\n See https://github.com/CrunchyData/crunchy-containers/blob/master/docs/metrics.adoc and https://github.com/CrunchyData/crunchy-containers/blob/master/docs/examples.adoc#metrics-collection for more details on setting up the crunchy-metrics solution.\n    "
},
{
	"uri": "/upgrading-the-operator/",
	"title": "Upgrading the Operator",
	"tags": [],
	"description": "",
	"content": "v2.6, 2018-04-25\n   Upgrading from v2.4 to v2.5   For a full list of additions and revisions that occurred in the PostgreSQL Operator v2.5 release, please view the related release page here.\n Required Updates This section notes some required steps that will need to be taken in the process of upgrading from v2.4 to v2.5.\n Configuration File It will be necessary to update your existing pgo.yaml configuration file where the Storage Configuration sections are concerned. The updated file for v2.5 can be found here. The file contained within the local installation of the Operator is located by default in the following location -\n $COROOT/conf/apiserver/pgo.yaml    Secrets 2.5 changed the names of the database credentials that are created by default in order to be consistent with the way new database credentials are named.\n It will be necessary to run the following script to update your existing clusters. This script will essentially copy the existing secrets values and create new secrets with those same values but named to the new standard. Run the script by passing in the name of an existing cluster as a parameter.\n $COROOT/bin/upgrade-secret.sh           Upgrading from v2.5 to v2.6   For a full list of additions and revisions that occurred in the PostgreSQL Operator v2.5 release, please view the related release page here.\n Required Updates This section notes some required steps that will need to be taken in the process of upgrading from v2.5 to v2.6.\n Configuration File One update in v2.6 changed the pgo.yaml file through removing the Debug flag. The Pgo.Debug variable can now be removed from the pgo.yaml file as a result. The debug flag is now called CRUNCHY_DEBUG and is set in the deployment.json file as a default environment variable.\n  Container Resources Release 2.6 added the concept of container resource configurations to the pgo.yaml file. In order to specify the optional container resource configurations, add a section as follows to your pgo.yaml file -\n DefaultContainerResource: small ContainerResources: small: RequestsMemory: 2Gi RequestsCPU: 0.5 LimitsMemory: 2Gi LimitsCPU: 1.0 large: RequestsMemory: 8Gi RequestsCPU: 2.0 LimitsMemory: 12Gi LimitsCPU: 4.0   \u0026lt;div class=\"notices warning\" \u0026gt;\u0026lt;div class=\"paragraph\"\u0026gt; \u0026lt;p\u0026gt;If these settings are set incorrectly or if the Kubernetes cluster cannot meet the defined memory and CPU requirements, deployments will go into a \u0026lt;strong\u0026gt;pending\u0026lt;/strong\u0026gt; state.\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;\n  Kube RBAC Release 2.6 added a rbac.yaml file to capture the Kube RBAC rules. These RBAC rules allow the apiserver and postgres-operator containers access to the Kubernetes resources required for the operator to work. As part of the deployment process, it is necessary to execute the rbac.yaml file to set the roles and bindings required by the operator. Adjust this file to suit local security requirements.\n  Application RBAC Release 2.6 added an RBAC capability to secure the pgo application. The pgouser now has a role appended at the end of of each user definition as follows -\n username:password:pgoadmin testuser:testpass:pgoadmin readonlyuser:testpass:pgoreader   These are defined in the following file -\n $COROOT/conf/apiserver/pgouser   To match the behavior of the pre 2.6 releases, the pgadmin role is set on the previous user definitions, but a readonlyuser is now defined to test other role definitions. The roles are defined in a new file called pgorole. This file defines each role and the permissions for that role. By default, two roles are defined as samples -\n pgoadmin pgoreader   Adjust these default settings to meet local security requirements.\n The format of this file is as follows -\n rolename: permissionA, permissionB   These are defined in the following file -\n $COROOT/conf/apiserver/pgorole   The complete set of permissions is documented in the Configuration document.\n  User Creation Release 2.6 replaced the pgo user --add command with the pgo create user command to improve consistency across command usage. Any scripts written using the older style of command require an update to use the new command syntax.\n  Replica CRD There is a new Kubernetes Custom Resource Definition that serves the purpose of holding replica information, called pgreplicas. This CRD is populated with the pgo scale command and is used to hold per-replica specific information such as the resource and storage configurations requested at run time.\n     \n "
},
{
	"uri": "/installation/",
	"title": "Installation",
	"tags": [],
	"description": "",
	"content": "v2.6, 2018-04-25\n   Quick Installation   Overview There are currently quickstart scripts that seek to automate the deployment to popular Kube environments -\n   quickstart-for-gke.sh\n  quickstart-for-ocp.sh\n   The quickstart-for-gke script will deploy the operator to a GKE Kube cluster.\n The quickstart-for-ocp script will deploy the operator to an Openshift Container Platform cluster.\n Both scripts assume you have a StorageClass defined for persistence.\n Pre-compiled versions of the Operator pgo client are provided for the x86_64, Mac OSX, and Windows hosts.\n   Quickstart GKE/PKS The quickstart-for-gke.sh script will allow users to set up the Postgres Operator quickly on GKE including PKS. This script is tested on GKE but can be modified for use with other Kubernetes environments as well.\n The script requires a few things in order to work -\n   wget utility installed\n  kubectl utility installed\n  StorageClass defined\n   Executing the script will give you a default Operator deployment that assumes dynamic storage and a storage class named standard, things that GKE provides.\n The script performs the following -\n   downloads the Operator configuration files\n  sets the $HOME/.pgouser file to default settings\n  deploys the Operator Deployment\n  sets your .bashrc to include the Operator environment variables\n  sets your $HOME/.bash_completion file to be the pgo bash_completion file\n    Openshift Container Platform A similar script for installing the operator on OCP is offered with similar features as the GKE script. This script is tested on OCP 3.7 with a StorageClass defined.\n         Manual Installation   Project Structure To perform an installation of the operator, first create the project structure as follows on your host, here we assume a local directory called odev -\n export GOPATH=$HOME/odev mkdir -p $HOME/odev/src $HOME/odev/bin $HOME/odev/pkg mkdir -p $GOPATH/src/github.com/crunchydata/   Next, get a tagged release of the source code -\n cd $GOPATH/src/github.com/crunchydata git clone https://github.com/CrunchyData/postgres-operator.git cd postgres-operator git checkout 2.6     Installation Prerequsites To run the operator and the pgo client, you will need the following -\n   a running Kubernetes or OpenShift cluster\n  the kubectl or oc clients installed in your PATH and configured to connect to the cluster (e.g. export KUBECONFIG=/etc/kubernetes/admin.conf)\n  a Kubernetes namespace created and set to where you want the operator installed. For this install we assume a namespace of demo has been created.\n   kubectl create -f examples/demo-namespace.json kubectl config set-context $(kubectl config current-context) --namespace=demo kubectl config view | grep namespace     Warning  If you are not using the demo namespace, it will be required to edit the following and change the namespace where the service account and cluster role bindings will be deployed.\n $COROOT/deploy/service-account.yaml\n $COROOT/deploy/cluster-role-binding.yaml\n     Permissions are granted to the Operator by means of a Service Account called postgres-operator. That service account is added to the Operator deployment.\n The postgres-operator service account is granted cluster-admin priviledges using a cluster role binding postgres-operator-cluster-role-binding.\n See here for more details on how to enable RBAC roles and modify the scope of the permissions to suit your needs.\n   Basic Installation The basic installation uses the default operator configuration settings, these settings assume you want to use HostPath storage on your Kube cluster for database persistence. Other persistent options are available but require the Advanced Installation below.\n Create HostPath Directory The default Persistent Volume script assumes a default HostPath directory be created called /data:\n sudo mkdir /data sudo chown 777 /data   Create some sample Persistent Volumes using the following script:\n export CO_NAMESPACE=demo export CO_CMD=kubectl export COROOT=$GOPATH/src/github.com/crunchydata/postgres-operator go get github.com/blang/expenv $COROOT/pv/create-pv.sh      Build Images \u0026amp; Deploy \u0026lt;div class=\"expand\"\u0026gt; \u0026lt;div class=\"expand-label\" style=\"cursor: pointer;\" onclick=\"$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fa fa-chevron-down' : 'fa fa-chevron-right';});});\"\u0026gt; \u0026lt;i style=\"font-size:x-small;\" class=\"fa fa-chevron-right\"\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;span\u0026gt;\n Packaged Images    \u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"expand-content\" style=\"display: none;\"\u0026gt; \u0026lt;div class=\"sect2\"\u0026gt; \u0026lt;h3 id=\"_packaged_images\"\u0026gt;Packaged Images\u0026lt;/h3\u0026gt; \u0026lt;div class=\"paragraph\"\u0026gt; \u0026lt;p\u0026gt;To pull prebuilt versions from Dockerhub of the \u0026lt;strong\u0026gt;postgres-operator\u0026lt;/strong\u0026gt; containers, specify the image versions, and execute the following Makefile target -\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"literalblock\"\u0026gt; \u0026lt;div class=\"content\"\u0026gt; \u0026lt;pre\u0026gt;export CO_IMAGE_PREFIX=crunchydata export CO_IMAGE_TAG=centos7-2.6 make pull\u0026lt;/pre\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"paragraph\"\u0026gt; \u0026lt;p\u0026gt;To pull down the prebuilt \u0026lt;strong\u0026gt;pgo\u0026lt;/strong\u0026gt; binaries, download the \u0026lt;strong\u0026gt;tar.gz\u0026lt;/strong\u0026gt; release file from the following link -\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"ulist\"\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt; \u0026lt;p\u0026gt;\u0026lt;a href=\"https://github.com/CrunchyData/postgres-operator/releases\"\u0026gt;Github Releases\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;p\u0026gt;extract (e.g. tar xvzf postgres-operator.2.6-rc1.tar.gz)\u0026lt;/p\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"literalblock\"\u0026gt; \u0026lt;div class=\"content\"\u0026gt; \u0026lt;pre\u0026gt;cd $HOME tar xvzf ./postgres-operator.2.6-rc1.tar.gz\u0026lt;/pre\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"ulist\"\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt; \u0026lt;p\u0026gt;copy \u0026lt;strong\u0026gt;pgo\u0026lt;/strong\u0026gt; client to somewhere in your path (e.g. cp pgo /usr/local/bin)\u0026lt;/p\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"paragraph\"\u0026gt; \u0026lt;p\u0026gt;Next, deploy the operator to your Kubernetes cluster -\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"literalblock\"\u0026gt; \u0026lt;div class=\"content\"\u0026gt; \u0026lt;pre\u0026gt;cd $COROOT make deployoperator\u0026lt;/pre\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;    \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;   \u0026lt;div class=\"expand\"\u0026gt; \u0026lt;div class=\"expand-label\" style=\"cursor: pointer;\" onclick=\"$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fa fa-chevron-down' : 'fa fa-chevron-right';});});\"\u0026gt; \u0026lt;i style=\"font-size:x-small;\" class=\"fa fa-chevron-right\"\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;span\u0026gt;\n Build from Source    \u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"expand-content\" style=\"display: none;\"\u0026gt; \u0026lt;div class=\"sect2\"\u0026gt; \u0026lt;h3 id=\"_build_from_source\"\u0026gt;Build from Source\u0026lt;/h3\u0026gt; \u0026lt;div class=\"paragraph\"\u0026gt; \u0026lt;p\u0026gt;The purpose of this section is to illustrate how to build the PostgreSQL Operator from source. These are considered advanced installation steps and should be primarily used by developers or those wishing a more precise installation method.\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"sect3\"\u0026gt; \u0026lt;h4 id=\"_requirements\"\u0026gt;Requirements\u0026lt;/h4\u0026gt; \u0026lt;div class=\"paragraph\"\u0026gt; \u0026lt;p\u0026gt;The postgres-operator runs on any Kubernetes and Openshift platform that supports Custom Resource Definitions. The Operator is tested on Kubeadm and OpenShift Container Platform environments.\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"paragraph\"\u0026gt; \u0026lt;p\u0026gt;The operator is developed with the Golang versions greater than or equal to version 1.8. See \u0026lt;a href=\"https://golang.org/dl/\"\u0026gt;Golang website\u0026lt;/a\u0026gt; for details on installing golang.\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"paragraph\"\u0026gt; \u0026lt;p\u0026gt;The Operator project builds and operates with the following containers -\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"ulist\"\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt; \u0026lt;p\u0026gt;\u0026lt;a href=\"https://hub.docker.com/r/crunchydata/pgo-lspvc/\"\u0026gt;PVC Listing Container\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;p\u0026gt;\u0026lt;a href=\"https://hub.docker.com/r/crunchydata/pgo-rmdata/\"\u0026gt;Remove Data Container\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;p\u0026gt;\u0026lt;a href=\"https://hub.docker.com/r/crunchydata/postgres-operator/\"\u0026gt;postgres-operator Container\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;p\u0026gt;\u0026lt;a href=\"https://hub.docker.com/r/crunchydata/pgo-apiserver/\"\u0026gt;apiserver Container\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;p\u0026gt;\u0026lt;a href=\"https://hub.docker.com/r/crunchydata/pgo-load/\"\u0026gt;file load Container\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"paragraph\"\u0026gt; \u0026lt;p\u0026gt;This Operator is developed and tested on the following operating systems but is known to run on other operating systems -\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"ulist\"\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt; \u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;CentOS 7\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;RHEL 7\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"paragraph\"\u0026gt; \u0026lt;p\u0026gt;First, install the project library dependencies. The godep dependency manager is used for this purpose. -\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"literalblock\"\u0026gt; \u0026lt;div class=\"content\"\u0026gt; \u0026lt;pre\u0026gt;cd $COROOT make setup\u0026lt;/pre\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"paragraph\"\u0026gt; \u0026lt;p\u0026gt;Then, compile the PostgreSQL Operator using the Makefile.\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"literalblock\"\u0026gt; \u0026lt;div class=\"content\"\u0026gt; \u0026lt;pre\u0026gt;cd $COROOT make all which pgo\u0026lt;/pre\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"paragraph\"\u0026gt; \u0026lt;p\u0026gt;Finally, deploy the operator to your Kubernetes cluster.\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"literalblock\"\u0026gt; \u0026lt;div class=\"content\"\u0026gt; \u0026lt;pre\u0026gt;cd $COROOT make deployoperator\u0026lt;/pre\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;    \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;   \u0026lt;div class=\"expand\"\u0026gt; \u0026lt;div class=\"expand-label\" style=\"cursor: pointer;\" onclick=\"$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fa fa-chevron-down' : 'fa fa-chevron-right';});});\"\u0026gt; \u0026lt;i style=\"font-size:x-small;\" class=\"fa fa-chevron-right\"\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;span\u0026gt;\n Makefile Targets    \u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"expand-content\" style=\"display: none;\"\u0026gt; \u0026lt;div class=\"sect1\"\u0026gt; \u0026lt;h2 id=\"_makefile_targets\"\u0026gt;Makefile Targets\u0026lt;/h2\u0026gt; \u0026lt;div class=\"sectionbody\"\u0026gt; \u0026lt;div class=\"paragraph\"\u0026gt; \u0026lt;p\u0026gt;The following table describes the Makefile targets: .Makefile Targets\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;table class=\"tableblock frame-topbot grid-all\" style=\"width: 80%;\"\u0026gt; \u0026lt;colgroup\u0026gt; \u0026lt;col style=\"width: 50%;\"\u0026gt; \u0026lt;col style=\"width: 50%;\"\u0026gt; \u0026lt;/colgroup\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th class=\"tableblock halign-left valign-top\"\u0026gt;Target\u0026lt;/th\u0026gt; \u0026lt;th class=\"tableblock halign-left valign-top\"\u0026gt;Description\u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;all\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;compile all binaries and build all images\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;setup\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;fetch the dependent packages required to build with\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;deployoperator\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;deploy the Operator (apiserver and postgers-operator) to Kubernetes\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;main\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;compile the postgres-operator\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;runmain\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;locally execute the postgres-operator\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;pgo\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;build the pgo binary\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;runpgo\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;run the pgo binary\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;runapiserver\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;run the apiserver binary outside of Kube\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;clean\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;remove binaries and compiled packages, restore dependencies\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;operatorimage\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;compile and build the postgres-operator Docker image\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;apiserverimage\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;compile and build the apiserver Docker image\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;lsimage\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;build the lspvc Docker image\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;loadimage\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;build the file load Docker image\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;rmdataimage\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;build the data deletion Docker image\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;release\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td class=\"tableblock halign-left valign-top\"\u0026gt;\u0026lt;p class=\"tableblock\"\u0026gt;build the postgres-operator release\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/tbody\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;    \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;   \u0026lt;div class=\"expand\"\u0026gt; \u0026lt;div class=\"expand-label\" style=\"cursor: pointer;\" onclick=\"$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fa fa-chevron-down' : 'fa fa-chevron-right';});});\"\u0026gt; \u0026lt;i style=\"font-size:x-small;\" class=\"fa fa-chevron-right\"\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;span\u0026gt;\n Helm Chart    \u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"expand-content\" style=\"display: none;\"\u0026gt; \u0026lt;div class=\"sect2\"\u0026gt; \u0026lt;h3 id=\"_helm_chart\"\u0026gt;Helm Chart\u0026lt;/h3\u0026gt; \u0026lt;div class=\"paragraph\"\u0026gt; \u0026lt;p\u0026gt;First, pull prebuilt versions from Dockerhub of the \u0026lt;strong\u0026gt;postgres-operator\u0026lt;/strong\u0026gt; containers, specify the image versions, and execute the following Makefile target -\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"literalblock\"\u0026gt; \u0026lt;div class=\"content\"\u0026gt; \u0026lt;pre\u0026gt;export CO_IMAGE_PREFIX=crunchydata export CO_IMAGE_TAG=centos7-2.6 make pull\u0026lt;/pre\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"paragraph\"\u0026gt; \u0026lt;p\u0026gt;Then, build and deploy the operator using the provided Helm chart -\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"literalblock\"\u0026gt; \u0026lt;div class=\"content\"\u0026gt; \u0026lt;pre\u0026gt;cd $COROOT/chart helm install ./postgres-operator helm ls\u0026lt;/pre\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;    \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;   Verify Operator Status To verify that the operator is deployed and running, run the following:\n kubectl get pod --selector=name=postgres-operator   You should see output similar to this:\n NAME READY STATUS RESTARTS AGE postgres-operator-56598999cd-tbg4w 2/2 Running 0 1m   There are 2 containers in the operator pod, both should be ready as above.\n When you first run the operator, it will create the required CustomResourceDefinitions. You can view these as follows -\n kubectl get crd   The operator creates the following Custom Resource Definitions over time as the associated commands are triggered.\n kubectl get crd NAME AGE pgbackups.cr.client-go.k8s.io 2d pgclusters.cr.client-go.k8s.io 2d pgingests.cr.client-go.k8s.io 2d pgpolicies.cr.client-go.k8s.io 2d pgreplicas.cr.client-go.k8s.io 2d pgtasks.cr.client-go.k8s.io 2d pgupgrades.cr.client-go.k8s.io 2d   At this point, the server side of the operator is deployed and ready.\n The complete set of environment variables used in the installation so far are -\n export CO_IMAGE_PREFIX=crunchydata export CO_IMAGE_TAG=centos7-2.6 export GOPATH=$HOME/odev export GOBIN=$GOPATH/bin export PATH=$PATH:$GOBIN export COROOT=$GOPATH/src/github.com/crunchydata/postgres-operator export CO_CMD=kubectl   You would normally add these into your .bashrc at this point to be used later on or if you want to redeploy the operator.\n  Configure pgo Client The pgo command line client requires TLS for securing the connection to the operator\u0026#8217;s REST API. This configuration is performed as follows:\n export PGO_CA_CERT=$COROOT/conf/apiserver/server.crt export PGO_CLIENT_CERT=$COROOT/conf/apiserver/server.crt export PGO_CLIENT_KEY=$COROOT/conf/apiserver/server.key   The pgo client uses Basic Authentication to authenticate to the operator REST API, for authentication, add the following .pgouser file to your $HOME:\n echo \"username:password\" \u0026gt; $HOME/.pgouser   The pgo client needs the URL to connect to the operator.\n Depending on your Kube environment this can be done the following ways:\n Running Kube Locally If your local host is not set up to resolve Kube Service DNS names, you can specify the operator IP address as follows:\n kubectl get service postgres-operator NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE postgres-operator NodePort 10.109.184.8 \u0026lt;none\u0026gt; 8443:30894/TCP 5m export CO_APISERVER_URL=https://10.109.184.8:8443 pgo version   You can also define a bash alias like:\n alias setip='export CO_APISERVER_URL=https://`kubectl get service postgres-operator -o=jsonpath=\"{.spec.clusterIP}\"`:8443'   This alias will set the CO_APISERVER_URL IP address for you!\n  Running Kube Remotely Set up a port-forward tunnel from your host to the Kube remote host, specifying the operator pod:\n kubectl get pod --selector=name=postgres-operator NAME READY STATUS RESTARTS AGE postgres-operator-56598999cd-tbg4w 2/2 Running 0 8m kubectl port-forward postgres-operator-56598999cd-tbg4w 8443:8443   In another terminal:\n export CO_APISERVER_URL=https://127.0.0.1:8443 pgo version     Verify pgo Client At this point you should be able to connect to the operator as follows:\n pgo version pgo client version 2.6 apiserver version 2.6   pgo commands are documented on the Getting Started page.\n    Storage Configuration Most users after they try out the operator will want to create a more customized installation and deployment of the operator using specific storage types.\n The operator will work with HostPath, NFS, Dynamic, and GKE Storage.\n NFS To configure the operator to use NFS for storage, a sample pgo.yaml.nfs file is provided. Overlay the default pgo.yaml file with that file:\n cp $COROOT/examples/pgo.yaml.nfs $COROOT/conf/apiserver/pgo.yaml   Edit the pgo.yaml file to specify the NFS GID that is set for the NFS volume mount you will be using, the default value assumed is nfsnobody as the GID (65534). Update the value to meet your NFS security settings.\n There is currently no script available to create your NFS Persistent Volumes but you can typically modify the $COROOT/pv/create-pv.sh script to work with NFS.\n  Dynamic To configure the operator to use Dynamic Storage classes for storage, a sample pgo.yaml.storageclass file is provided. Overlay the default pgo.yaml file with that file:\n cp $COROOT/examples/pgo.yaml.storageclass $COROOT/conf/apiserver/pgo.yaml   Edit the pgo.yaml file to specify the storage class you will be using, the default value assumed is standard which is the name used by default within a GKE Kube cluster deployment. Update the value to match your storage classes.\n Notice that the FsGroup setting is required for most block storage and is set to the value of 26 since the PostgreSQL container runs as UID 26.\n  GKE Some notes for setting up GKE for the Operator deployment.\n Install Kubectl On your host you will be working from, install the kubectl command:\n https://kubernetes.io/docs/tasks/tools/install-kubectl/\n  GCP   Select your project\n  Create a Kube cluster in that project\n   By default a storage class called standard is created.\n  Install GCloud To access the Kube cluster you need to install the gcloud utility:\n https://cloud.google.com/sdk/downloads cd google-cloud-sdk ./install.sh    Configure Kubectl for Cluster Access gcloud auth login gcloud container clusters get-credentials jeff-quickstart --zone us-central1-a --project crunchy-dev-test kubectl get storageclass        \n   I\u0026#39;ve installed the Operator. Now what?   Next Steps There are many ways to configure the operator further. Some sample configurations are documented on the Configuration page.\n You may also want to find out more information on how the operator is designed to work and deploy. This information can be found in the How It Works page.\n Information can be found on the full scope of commands on the Getting Started page.\n    \n "
},
{
	"uri": "/configuration/",
	"title": "Configuration",
	"tags": [],
	"description": "",
	"content": "Table of Contents Overview Openshift Container Platform Security Configuration Kube RBAC Basic Authentication Configure TLS pgo RBAC apiserver Configuration postgres-operator Container Configuration   bash Completion REST API Deploying pgpool   v2.6, 2018-04-25\n Overview This document describes how to configure the operator beyond the default configurations and what the configuration settings mean.\n   Openshift Container Platform To run the Operator on Openshift Container Platform note the following:\n   Openshift Container Platform 3.7 or greater is required since the Operator is based on Custom Resource Definitions which were first supported in OCP starting with version 3.7\n  the OC_CMD environment variable should be set to oc when operating in an Openshift environment\n     Security Configuration Kube RBAC The apiserver and postgres-operator containers access Kube resources and need priviledges for interacting with Kube. The rbac.yaml file includes a set of roles and bindings that allow the operator to work. These are fine grained controls that you can adjust to your local Kube cluster depending on your security requirements\n The rbac.yaml file gets executed when you deploy the operator to your Kube cluster.\n Permissions are granted to the Operator by means of a Service Account called postgres-operator. That service account is added to the Operator deployment.\n If you are not using the demo namespace, you will edit the following:\n   $COROOT/deploy/service-account.yaml\n   See here for more details on how to enable RBAC roles and modify the scope of the permissions to suit your needs.\n  Basic Authentication Basic Authentication is required by the apiserver. You will configure the pgo client to specify a basic authentication username and password by creating a file in the user\u0026#8217;s home directory named .pgouser that looks similar to this, containing only a single line:\n username:password   This example specifies a username of username and a password of password. These values will be read by the pgo client and passed to the apiserver on each REST API call.\n For the apiserver, a list of usernames and passwords is specified in the apiserver-conf-secret Secret. The values specified in a deployment are found in the following location:\n $COROOT/conf/apiserver/pgouser   The sample configuration for pgouser is as follows:\n username:password testuser:testpass   Modify these values to be unique to your environment.\n If the username and password passed by clients to the apiserver do not match, the REST call will fail and a log message will be produced in the apiserver container log. The client will receive a 401 http status code if they are not able to authenticate.\n If the pgouser file is not found in the home directory of the pgo user then the next searched location is /etc/pgo/pgouser, and if not found there then lastly the PGOUSER environment variable is searched for a path to the basic authentication file.\n You can turn off Basic Authentication entirely if you set the BasicAuth setting in the pgo.yaml configuration file to false.\n  Configure TLS TLS is used to secure communications to the apiserver. Sample keys/certs used by TLS are found here:\n $COROOT/conf/apiserver/server.crt $COROOT/conf/apiserver/server.key   If you want to generate your own keys, you can use the script found in:\n $COROOT/bin/make-certs.sh   The pgo client is required to use keys to connect to the apiserver. Specify the keys to pgo by setting the following environment variables:\n export PGO_CA_CERT=$COROOT/conf/apiserver/server.crt export PGO_CLIENT_CERT=$COROOT/conf/apiserver/server.crt export PGO_CLIENT_KEY=$COROOT/conf/apiserver/server.key   The sample server keys are used as the client keys, adjust to suit your requirements.\n For the apiserver TLS configuration, the keys are included in the apiserver-conf-secret Secret when the apiserver is deployed. See the $COROOT/deploy/deploy.sh script which is where the secret is created.\n The apiserver listens on port 8443 (e.g. https://postgres-operator:8443).\n You can set InsecureSkipVerify to true if you set the NO_TLS_VERIFY environment variable in the deployment.json file to true. By default this value is set to false if you do not specify a value.\n  pgo RBAC The pgo command line utility talks to the apiserver REST API instead of the Kube API. Therefore it requires its own RBAC configuration.\n Starting in Release 2.6, the /conf/apiserver/pgorole is used to define some sample pgo roles, pgadmin and pgoreader.\n These roles are meant as samples that you can configure to suite your own security requirements. The pgadmin role grants a user authorization to all pgo commands. The pgoreader only grants access to pgo commands that display information such as pgo show cluster.\n The pgorole file is read at start up time when the operator is deployed to your Kube cluster.\n Also, the pguser file now includes the role that is assigned to a specific user as follows:\n username:password:pgoadmin testuser:testpass:pgoadmin readonlyuser:testpass:pgoreader   The following list shows the current pgo permissions: .pgo Permissions\n     Permission Description     ShowCluster\n allow pgo show cluster\n   CreateCluster\n allow pgo create cluster\n   TestCluster\n allow pgo test mycluster\n   ShowBackup\n allow pgo show backup\n   CreateBackup\n allow pgo backup mycluster\n   DeleteBackup\n allow pgo delete backup mycluster\n   Label\n allow pgo label\n   Load\n allow pgo load\n   CreatePolicy\n allow pgo create policy\n   DeletePolicy\n allow pgo delete policy\n   ShowPolicy\n allow pgo show policy\n   ApplyPolicy\n allow pgo apply policy\n   ShowPVC\n allow pgo show pvc\n   CreateUpgrade\n allow pgo upgrade\n   ShowUpgrade\n allow pgo show upgrade\n   DeleteUpgrade\n allow pgo delete upgrade\n   CreateUser\n allow pgo create user\n   CreateFailover\n allow pgo failover\n   User\n allow pgo user\n   Version\n allow pgo version\n    If you are not authorized for a pgo command the user will get back this response:\n FATA[0000] Authentication Failed: 40    apiserver Configuration The postgres-operator pod includes the apiserver which is a REST API that pgo users communicate with.\n The apiserver uses the following configuration files found in $COROOT/conf/apiserver to determine how the Operator will provision PostgreSQL containers:\n $COROOT/conf/apiserver/pgo.yaml $COROOT/conf/apiserver/pgo.lspvc-template.json $COROOT/conf/apiserver/pgo.load-template.json   Note that the default pgo.yaml file assumes you are going to use HostPath Persistent Volumes for your storage configuration. Adjust this file for NFS or other storage configurations.\n The version of PostgreSQL container the Operator will deploy is determined by the CCPImageTag setting in the $COROOT/conf/apiserver/pgo.yaml configuration file. By default, this value is set to the latest release of the Crunchy Container Suite.\n pgo.yaml The default pgo.yaml configuration file, included in $COROOT/conf/apiserver/pgo.yaml, looks like this:\n BasicAuth: true Cluster: CCPImageTag: centos7-10.3-1.8.2 Port: 5432 User: testuser Database: userdb PasswordAgeDays: 60 PasswordLength: 8 Strategy: 1 Replicas: 0 PrimaryStorage: storage1 BackupStorage: storage1 ReplicaStorage: storage1 Storage: storage1: AccessMode: ReadWriteMany Size: 200M StorageType: create storage2: AccessMode: ReadWriteMany Size: 333M StorageType: create storage3: AccessMode: ReadWriteMany Size: 440M StorageType: create DefaultContainerResource: small ContainerResources: small: RequestsMemory: 2Gi RequestsCPU: 0.5 LimitsMemory: 2Gi LimitsCPU: 1.0 large: RequestsMemory: 8Gi RequestsCPU: 2.0 LimitsMemory: 12Gi LimitsCPU: 4.0 Pgo: Audit: false Metrics: false LSPVCTemplate: /config/pgo.lspvc-template.json CSVLoadTemplate: /config/pgo.load-template.json COImagePrefix: crunchydata COImageTag: centos7-2.6   Values in the pgo configuration file have the following meaning:\n Table 1. pgo Configuration File Definitions     Setting Definition     BasicAuth\n if set to true will enable Basic Authentication\n   Cluster.CCPImageTag\n newly created containers will be based on this image version (e.g. centos7-10.3-1.8.1), unless you override it using the --ccp-image-tag command line flag\n   Cluster.Port\n the PostgreSQL port to use for new containers (e.g. 5432)\n   Cluster.User\n the PostgreSQL normal user name\n   Cluster.Strategy\n sets the deployment strategy to be used for deploying a cluster, currently there is only strategy 1\n   Cluster.Replicas\n the number of cluster replicas to create for newly created clusters\n   Cluster.Policies\n optional, list of policies to apply to a newly created cluster, comma separated, must be valid policies in the catalog\n   Cluster.PasswordAgeDays\n optional, if set, will set the VALID UNTIL date on passwords to this many days in the future when creating users or setting passwords, defaults to 60 days\n   Cluster.PasswordLength\n optional, if set, will determine the password length used when creating passwords, defaults to 8\n   PrimaryStorage\n required, the value of the storage configuration to use for the primary PostgreSQL deployment\n   BackupStorage\n required, the value of the storage configuration to use for backups\n   ReplicaStorage\n required, the value of the storage configuration to use for the replica PostgreSQL deployments\n   Storage.storage1.StorageClass\n for a dynamic storage type, you can specify the storage class used for storage provisioning(e.g. standard, gold, fast)\n   Storage.storage1.AccessMode\n the access mode for new PVCs (e.g. ReadWriteMany, ReadWriteOnce, ReadOnlyMany). See below for descriptions of these.\n   Storage.storage1.Size\n the size to use when creating new PVCs (e.g. 100M, 1Gi)\n   Storage.storage1.StorageType\n supported values are either dynamic, existing, create, or emptydir, if not supplied, emptydir is used\n   Storage.storage1.Fsgroup\n optional, if set, will cause a SecurityContext and fsGroup attributes to be added to generated Pod and Deployment definitions\n   Storage.storage1.SupplementalGroups\n optional, if set, will cause a SecurityContext to be added to generated Pod and Deployment definitions\n   DefaultContainerResource\n optional, the value of the container resources configuration to use for all database containers, if not set, no resource limits or requests are added on the database container\n   ContainerResources.small.RequestsMemory\n request size of memory in bytes\n   ContainerResources.small.RequestsCPU\n request size of CPU cores\n   ContainerResources.small.LimitsMemory\n request size of memory in bytes\n   ContainerResources.small.LimitsCPU\n request size of CPU cores\n   ContainerResources.large.RequestsMemory\n request size of memory in bytes\n   ContainerResources.large.RequestsCPU\n request size of CPU cores\n   ContainerResources.large.LimitsMemory\n request size of memory in bytes\n   ContainerResources.large.LimitsCPU\n request size of CPU cores\n   Pgo.LSPVCTemplate\n the PVC lspvc template file that lists PVC contents\n   Pgo.LoadTemplate\n the load template file used for load jobs\n   Pgo.COImagePrefix\n image tag prefix to use for the Operator containers\n   Pgo.COImageTag\n image tag to use for the Operator containers\n   Pgo.Audit\n boolean, if set to true will cause each apiserver call to be logged with an audit marking\n   Pgo.Metrics\n boolean, if set to true will cause each new cluster to include crunchy-collect as a sidecar container for metrics collection, if set to false (default), users can still add metrics on a cluster-by-cluster basis using the pgo command flag --metrics\n     Storage Configurations You can now define n-number of Storage configurations within the pgo.yaml file. Those Storage configurations follow these conventions:\n   they must have lowercase name (e.g. storage1)\n  they must be unique names (e.g. mydrstorage, faststorage, slowstorage)\n   These Storage configurations are referenced in the BackupStorage, ReplicaStorage, and PrimaryStorage configuration values. However, there are command line options in the pgo client that will let a user override these default global values to offer you the user a way to specify very targeted storage configurations when needed (e.g. disaster recovery storage for certain backups).\n You can set the storage AccessMode values to the following:\n   ReadWriteMany - mounts the volume as read-write by many nodes\n  ReadWriteOnce - mounts the PVC as read-write by a single node\n  ReadOnlyMany - mounts the PVC as read-only by many nodes\n   These Storage configurations are validated when the pgo-apiserver starts, if a non-valid configuration is found, the apiserver will abort. These Storage values are only read at apiserver start time.\n The following StorageType values are possible:\n   dynamic - this will allow for dynamic provisioning of storage using a StorageClass.\n  existing - This setting allows you to use a PVC that already exists. For example, if you have a NFS volume mounted to a PVC, all PostgreSQL clusters can write to that NFS volume mount via a common PVC. When set, the Name setting is used for the PVC.\n  create - This setting allows for the creation of a new PVC for each PostgreSQL cluster using a naming convention of clustername-pvc*. When set, the Size, AccessMode settings are used in constructing the new PVC.\n  emptydir - If a StorageType value is not defined, emptydir is used by default. This is a volume type that’s created when a pod is assigned to a node and exists as long as that pod remains running on that node; it is deleted as soon as the pod is manually deleted or removed from the node.\n   The operator will create new PVCs using this naming convention: dbname-pvc where dbname is the database name you have specified. For example, if you run:\n pgo create cluster example1   It will result in a PVC being created named example1-pvc and in the case of a backup job, the pvc is named example1-backup-pvc\n There are currently 3 sample pgo configuration files provided for users to use as a starting configuration:\n   pgo.yaml.emptydir - this configuration specifies emptydir storage to be used for databases\n  pgo.yaml.nfs - this configuration specifies create storage to be used, this is used for NFS storage for example where you want to have a unique PVC created for each database\n  pgo.yaml.dynamic - this configuration specifies dynamic storage to be used, namely a storageclass that refers to a dynamic provisioning strorage such as StorageOS or Portworx, or GCE.\n    Overriding Container Resources Configuration Defaults In the pgo.yaml configuration file you have the option to configure a default container resources configuration that when set will add CPU and memory resource limits and requests values into each database container when the container is created.\n You can also override the default value using the --resources-config command flag when creating a new cluster:\n pgo create cluster testcluster --resources-config=large   Note, if you try to allocate more resources than your host or Kube cluster has available then you will see your pods wait in a Pending status. The output from a kubectl describe pod command will show output like this in this case:\n Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 49s (x8 over 1m) default-scheduler No nodes are available that match all of the predicates: Insufficient memory (1).    Overriding Storage Configuration Defaults pgo create cluster testcluster --storage-config=bigdisk   That example will create a cluster and specify a storage configuration of bigdisk to be used for the primary database storage, the replica storage will default to the value of ReplicaStorage as specified in pgo.yaml.\n pgo create cluster testcluster2 --storage-config=fastdisk --replica-storage-config=slowdisk   That example will create a cluster and specify a storage configuration of fastdisk to be used for the primary database storage, the replica storage will use the storage configuration slowdisk.\n pgo backup testcluster --storage-config=offsitestorage   That example will create a backup and use the offsitestorage storage configuration for persisting the backup.\n  Disaster Recovery Using Storage Configurations A simple mechanism for partial disaster recovery can be obtained by leveraging network storage, Kubernetes storage classes, and the storage configuration options within the Operator.\n For example, if you define a Kubernetes storage class that refers to a storage backend that is running within your disaster recovery site, and then use that storage class as a storage configuration for your backups, you essentially have moved your backup files automatically to your DR site thanks to network storage.\n     postgres-operator Container Configuration To enable debug level messages from the operator pod, set the CRUNCHY_DEBUG environment variable to true within its deployment file deployment.json.\n Operator Templates The database and cluster Kubernetes objects that get created by the operator are based on json templates that are added into the operator deployment by means of a mounted volume.\n The templates are located in the $COROOT/conf/postgres-operator directory and get added into a config map which is mounted by the operator deployment.\n     bash Completion There is a bash completion file that is included for users to try, this is located in the repository at example/pgo-bash-completion. To use it:\n cp $COROOT/example/pgo-bash-completion /etc/bash_completion.d/pgo su - $USER     REST API Because the apiserver implements a REST API, you can integrate with it using your own application code. To demonstrate this, the following curl commands show the API usage:\n pgo version\n curl -v -X GET -u readonlyuser:testpass -H \"Content-Type: application/json\" --insecure https://10.101.155.218:8443/version   pgo show policy all\n curl -v -X GET -u readonlyuser:testpass -H \"Content-Type: application/json\" --insecure https://10.101.155.218:8443/policies/all   pgo show pvc danger-pvc\n curl -v -X GET -u readonlyuser:testpass -H \"Content-Type: application/json\" --insecure https://10.101.155.218:8443/pvc/danger-pvc   pgo show cluster mycluster\n curl -v -X GET -u readonlyuser:testpass -H \"Content-Type: application/json\" --insecure https://10.101.155.218:8443/clusters/mycluster   pgo show upgrade mycluster\n curl -v -X GET -u readonlyuser:testpass -H \"Content-Type: application/json\" --insecure https://10.101.155.218:8443/upgrades/mycluster   pgo test mycluster\n curl -v -X GET -u readonlyuser:testpass -H \"Content-Type: application/json\" --insecure https://10.101.155.218:8443/clusters/test/mycluster   pgo show backup mycluster\n curl -v -X GET -u readonlyuser:testpass -H \"Content-Type: application/json\" --insecure https://10.101.155.218:8443/backups/mycluster     Deploying pgpool It is optional but you can cause a pgpool Deployment to be created as part of a Postgres cluster. Running pgpool only makes sense when you have both a primary and some number of replicas deployed as part of your Postgres cluster. The current pgpool configuration deployed by the operator only works when you have both a primary and replica running.\n When a user creates the cluster they can pass a command flag as follows:\n pgo create cluster cluster1 --pgpool pgo scale cluster1   This will cause the operator to create a Deployment that includes the crunchy-pgpool container along with a replica. That container will create a configuration that will perform SQL routing to your cluster services, both for the primary and replica services.\n Pgpool examines the SQL it receives and routes the SQL statement to either the primary or replica based on the SQL action specifically it will send writes and updates to only the primary service. It will send read-only statements to the replica service.\n When the operator deploys the pgpool container, it creates a secret (e.g. mycluster-pgpool-secret) that contains pgpool configuration files. It fills out templated versions of these configuration files specifically for this postgres cluster.\n Part of the pgpool deployment also includes creating a pool_passwd file that will allow the testuser credential to authenticate to pgpool. Adding additional users to the pgpool configuration currently requires human intervention specifically creating a new pgpool secret and bouncing the pgpool pod to pick up the updated secret. Future operator releases will attempt to provide pgo commands to let you automate the addition or removal of a pgpool user.\n Currently to update a pgpool user within the pool_passwd configuration file, you will have to copy the existing files from the secret to your local system, update the credentials in pool_passwd with your new user credentials, and then recreate the pgpool secret, and finally restart the pgpool pod to pick up the updated configuration files.\n Example:\n kubectl cp demo/wed10-pgpool-6cc6f6598d-wcnmf:/pgconf/ /tmp/foo   That command gets a running set of secret pgpool configuration files and places them locally on your system for you to edit.\n pgpool requires a specially formatted password credential to be placed into pool_passwd. There is a golang program included in $COROOT/golang-examples/gen-pgpool-pass.go* that when run will generate the value to use within the pgpool_passwd configuration file.\n go run $COROOT/golang-examples/gen-pgpool-pass.go Enter Username: testuser Enter Password: Password typed: e99Mjt1dLz hash of password is [md59c4017667828b33762665dc4558fbd76]   The value md59c4017667828b33762665dc4558fbd76 is what you will use in the pool_passwd file.\n Then, create the new secrets file based on those updated files:\n $COROOT/bin/create-pgpool-secrets.sh   Lastly for pgpool to pick up the new secret file, delete the existing deployment pod:\n kubectl get deployment wed-pgpool kubectl delete pod wed10-pgpool-6cc6f6598d-wcnmf   The pgpool deployment will spin up another pgpool which will pick up the updated secret file.\n   "
},
{
	"uri": "/getting-started/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": "Table of Contents pgo Commands pgo version pgo create cluster pgo backup pgo delete backup pgo delete cluster pgo scale pgo upgrade pgo delete upgrade pgo show pvc pgo show cluster pgo test pgo create policy pgo delete policy pgo apply pgo user pgo label pgo load pgo failover     v2.6, 2018-04-25\n pgo Commands Prior to using pgo, users will need to specify the postgres-operator URL as follows:\n kubectl get service postgres-operator NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE postgres-operator 10.104.47.110 \u0026lt;none\u0026gt; 8443/TCP 7m export CO_APISERVER_URL=https://10.104.47.110:8443 pgo version   pgo version To see what version of pgo client and postgres-operator you are running, use the following:\n pgo version    pgo create cluster To create a database, use the following:\n pgo create cluster mycluster   A more complex example is to create a series of clusters such as:\n pgo create cluster xraydb --series=3 --labels=project=xray --policies=xrayapp,rlspolicy   In the example above, we provision 3 clusters that have a number appended into their resulting cluster name, apply a user defined label to each cluster, and also apply user defined policies to each cluster after they are created.\n You can then view that database as:\n pgo show cluster mydatabase   Also, if you like to see JSON formatted output, add the -o json flag:\n pgo show cluster mydatabase -o json   The output will give you the current status of the database pod and the IP address of the database service. If you have postgresql installed on your test system you can connect to the database using the service IP address:\n psql -h 10.105.121.12 -U postgres postgres   More details are available on user management below, however, you may wish to take note that user credentials are created in the file $COROOT/deploy/create-secrets.sh upon deployment of the Operator. The following user accounts and passwords are created by default for connecting to the PostgreSQL clusters:\n username: postgres password: password\n username: primaryuser password: password\n username: testuser password: password\n You can view all databases using the special keyword all:\n pgo show cluster all   You can filter the results based on the Postgres Version:\n pgo show cluster all --version=9.6.2   You can also add metrics collection to a cluster by using the --metrics command flag as follows:\n pgo create cluster testcluster --metrics   This command flag causes a crunchy-collect container to be added to the database cluster pod and enables metrics collection on that database pod. For this to work, you will need to configure the Crunchy metrics example as found in the Crunchy Container Suite.\n New clusters typically pick up the container image version to use based on the pgo configuration file\u0026#8217;s CCP_IMAGE_TAG setting. You can override this value using the --ccp-image-tag command line flag:\n pgo create cluster mycluster --ccp-image-tag=centos7-9.6.5-1.6.0   You can also add a pgpool deployment into a cluster by using the --pgpool command flag as follows:\n pgo create cluster testcluster --pgpool   This will cause a crunchy-pgpool container to be started and initially configured for a cluster and the testuser cluster credential. See below for more details on running a pgpool deployment as part of your cluster.\n  pgo backup You can start a backup job for a cluster as follows:\n pgo backup mycluster   You can view the backup:\n pgo show backup mycluster   View the PVC folder and the backups contained therein:\n pgo show pvc mycluster-backup-pvc pgo show pvc mycluster-backup-pvc --pvc-root=mycluster-backups   The output from this command is important in that it can let you copy/paste a backup snapshot path and use it for restoring a database or essentially cloning a database with an existing backup archive.\n For example, to restore a database from a backup archive:\n pgo create cluster restoredb --backup-path=mycluster-backups/2017-03-27-13-56-49 --backup-pvc=mycluster-pvc --secret-from=mycluster   This will create a new database called restoredb based on the backup found in mycluster-backups/2017-03-27-13-56-49 and the secrets of the mycluster cluster.\n Selectors can be used to perform backups as well, for example:\n pgo backup --selector=project=xray   In this example, any cluster that matches the selector will cause a backup job to be created.\n When you request a backup, pgo will prompt you if you want to proceed because this action will delete any existing backup job for this cluster that might exist. The backup files will still be left intact but the actual Kubernetes Job will be removed prior to creating a new Job with the same name.\n  pgo delete backup To delete a backup enter the following:\n pgo delete backup mycluster    pgo delete cluster You can remove a cluster by running:\n pgo delete cluster restoredb   Note, that this command will not remove the PVC associated with this cluster.\n Selectors also apply to the delete command as follows:\n pgo delete cluster --selector=project=xray   This command will cause any cluster matching the selector to be removed.\n You can remove a cluster and it\u0026#8217;s data files by running:\n pgo delete cluster restoredb --delete-data   You can remove a cluster, it\u0026#8217;s data files, and all backups by running:\n pgo delete cluster restoredb --delete-data --delete-backups   When you specify a destructive delete like above, you will be prompted to make sure this is what you want to do. If you don\u0026#8217;t want to be prompted you can enter the --no-prompt command line flag.\n  pgo scale When you create a Cluster, you will see in the output a variety of Kubernetes objects were created including:\n   a Deployment holding the primary PostgreSQL database\n  a Deployment holding the replica PostgreSQL database\n  a service for the primary database\n  a service for the replica databases\n   Since Postgres is a single-primary database by design, the primary Deployment is set to a replica count of 1, it can not scale beyond 1.\n With Postgres, you can any n-number of replicas each of which connect to the primary forming a streaming replication postgres cluster. The Postgres replicas are read-only, whereas the primary is read-write. To create a Postgres replica enter a command such as:\n pgo scale mycluster   The pgo scale command is additive, in that each time you execute it, it will create another replica which is added to the Postgres cluster.\n There are 2 service connections available to the PostgreSQL cluster. One is to the primary database which allows read-write SQL processing, and the other is to the set of read-only replica databases. The replica service performs round-robin load balancing to the replica databases.\n You can connect to the primary database and verify that it is replicating to the replica databases as follows:\n psql -h 10.107.180.159 -U postgres postgres -c 'table pg_stat_replication'   You can view all clusters using the special keyword all:\n pgo show cluster all   You can filter the results by Postgres version:\n pgo show cluster all --version=9.6.2   The scale command will let you specify a --node-label flag which can be used to influence what Kube node the replica will be scheduled upon.\n pgo scale mycluster --node-label=speed=fast   If you don\u0026#8217;t specify a --node-label flag, a node affinity rule of NotIn will be specified to prefer that the replica be schedule on a node that the primary is not running on.\n You can also dictate what container resource and storage configurations will be used for a replica by passing in extra command flags:\n pgo scale mycluster --storage-config=storage1 --resources-config=small    pgo upgrade You can perform a minor Postgres version upgrade of either a database or cluster as follows:\n pgo upgrade mycluster   When you run this command, it will cause the operator to delete the existing containers of the database or cluster and recreate them using the currently defined Postgres container image specified in your pgo configuration file.\n The database data files remain untouched, only the container is updated, this will upgrade your Postgres server version only.\n You can perform a major Postgres version upgrade of either a database or cluster as follows:\n pgo upgrade mycluster --upgrade-type=major   When you run this command, it will cause the operator to delete the existing containers of the database or cluster and recreate them using the currently defined Postgres container image specified in your pgo configuration file.\n The database data files are converted to the new major Postgres version as specified by the current Postgres image version in your pgo configuration file.\n In this scenario, the upgrade is performed by the Postgres pg_upgrade utility which is containerized in the crunchydata/crunchy-upgrade container. The operator will create a Job which runs the upgrade container, using the existing Postgres database files as input, and output the updated database files to a new PVC.\n Once the upgrade job is completed, the operator will create the original database or cluster container mounted with the new PVC which contains the upgraded database files.\n As the upgrade is processed, the status of the pgupgrade CRD is updated to give the user some insight into how the upgrade is proceeding. Upgrades like this can take a long time if your database is large. The operator creates a watch on the upgrade job to know when and how to proceed.\n Likewise, you can upgrade the cluster using a command line flag:\n pgo upgrade mycluster --ccp-image-tag=centos7-9.6.8-1.8.1 pgo upgrade mycluster --upgrade-type=major --ccp-image-tag=centos7-9.6.8-1.8.1    pgo delete upgrade To remove an upgrade CRD, issue the following:\n pgo delete upgrade    pgo show pvc You can view the files on a PVC as follows:\n pgo show pvc mycluster-pvc   In this example, the PVC is mycluster-pvc. This command is useful in some cases to examine what files are on a given PVC.\n In the case where you want to list a specific path on a PVC you can specify the path option as follows:\n pgo show pvc mycluster-pvc --pvc-root=mycluster-backups   You can also list all PVCs that are created by the operator using:\n pgo show pvc all    pgo show cluster You can view the passwords used by the cluster as follows:\n pgo show cluster mycluster --show-secrets=true   Passwords are generated if not specified in your pgo configuration.\n  pgo test You can test the database connections to a cluster:\n pgo test mycluster   This command will test each service defined for the cluster using the postgres, primary, and normal user accounts defined for the cluster. The cluster credentials are accessed and used to test the database connections. The equivalent psql command is printed out as connections are tried, along with the connection status.\n Like other commands, you can use the selector to test a series of clusters:\n pgo test --selector=env=research pgo test all   You can get output using the --output flag:\n pgo test all -o json    pgo create policy To create a policy use the following syntax:\n pgo create policy policy1 --in-file=/tmp/policy1.sql pgo create policy policy1 --url=https://someurl/policy1.sql   When you execute this command, it will create a policy named policy1 using the input file /tmp/policy1.sql as input. It will create on the server a PgPolicy CRD with the name policy1 that you can examine as follows:\n kubectl get pgpolicies policy1 -o json   Policies get automatically applied to any cluster you create if you define in your pgo.yaml configuration a CLUSTER.POLICIES value. Policy SQL is executed as the postgres user.\n To view policies:\n pgo show policy all    pgo delete policy To delete a policy use the following form:\n pgo delete policy policy1    pgo apply To apply an existing policy to a set of clusters, issue a command like this:\n pgo apply policy1 --selector=name=mycluster   When you execute this command, it will look up clusters that have a label value of name=mycluster and then it will apply the policy1 label to that cluster and execute the policy SQL against that cluster using the postgres user account.\n Policies are executed as the superuser or postgres user in PostgreSQL. These should therefore be exercised with caution.\n   If you want to view the clusters than have a specific policy applied to them, you can use the --selector flag as follows to filter on a policy name (e.g. policy1):\n pgo show cluster --selector=policy1=pgpolicy    pgo user To create a new Postgres user to the mycluster cluster, execute:\n pgo createa user sally --selector=name=mycluster   To delete a Postgres user in the mycluster cluster, execute:\n pgo user --delete-user=sally --selector=name=mycluster   To delete that user in all clusters:\n pgo user --delete-user=sally   To change the password for a user in the mycluster cluster:\n pgo user --change-password=sally --selector=name=mycluster   The password is generated and applied to the user sally.\n To see user passwords that have expired past a certain number of days in the mycluster cluster:\n pgo user --expired=7 --selector=name=mycluster   To assign users to a cluster:\n pgo create user user1 --valid-days=30 --managed --db=userdb --selector=name=xraydb1   In this example, a user named user1 is created with a valid until password date set to expire in 30 days. That user will be granted access to the userdb database. This user account also will have an associated secret created to hold the password that was generated for this user. Any clusters that match the selector value will have this user created on it.\n To change a user password:\n pgo user --change-password=user1 --valid-days=10 --selector=name=xray1   In this example, a user named user1 has its password changed to a generated value and the valid until expiration date set to 10 days from now, this command will take effect across all clusters that match the selector. If you specify valid-days=-1 it will mean the password will not expire (e.g. infinity).\n To drop a user:\n pgo user --delete-user=user3 --selector=project=xray   To see which passwords are set to expire in a given number of days:\n pgo user --expired=10 --selector=project=xray   In this example, any clusters that match the selector are queried to see if any users are set to expire in 10 days.\n To update expired passwords in a cluster:\n pgo user --update-passwords --selector=name=mycluster    pgo label You can apply a user defined label to a cluster as follows:\n pgo label --label=env=research --selector=project=xray   In this example, we apply a label of env=research to any clusters that have an existing label of project=xray applied.\n  pgo load A CSV file loading capability is supported currently. You can test that by creating a SQL Policy which will create a database table that will be loaded with the CSV data. For example:\n pgo create policy xrayapp --in-file=$COROOT/examples/policy/xrayapp.sql   Then you can load a sample CSV file into a database as follows:\n pgo load --load-config=$COROOT/examples/sample-load-config.json --selector=name=mycluster   The loading is based on a load definition found in the sample-load-config.json file. In that file, the data to be loaded is specified. When the pgo load command is executed, Jobs will be created to perform the loading for each cluster that matches the selector filter.\n If you include the --policies flag, any specified policies will be applied prior to the data being loaded. For example:\n pgo load --policies=\"rlspolicy,xrayapp\" --load-config=$COROOT/examples/sample-load-config.json --selector=name=mycluster   Likewise you can load a sample json file into a database as follows:\n pgo load --policies=jsonload --load-config=$COROOT/examples/sample-json-load-config.json --selector=name=mycluster   The load configuration file has the following YAML attributes:\n Table 1. Load Configuration File Definitions     Attribute Description     COImagePrefix\n the pgo-load image prefix to use for the load job\n   COImageTag\n the pgo-load image tag to use for the load job\n   DbDatabase\n the database schema to use for loading the data\n   DbUser\n the database user to use for loading the data\n   DbPort\n the database port of the database to load\n   TableToLoad\n the PostgreSQL table to load\n   FilePath\n the name of the file to be loaded\n   FileType\n either csv or json, determines the type of data to be loaded\n   PVCName\n the name of the PVC that holds the data file to be loaded\n   SecurityContext\n either fsGroup or SupplementalGroup values\n     pgo failover Starting with Release 2.6, there is a manual failover command which can be used to promote a replica to a primary role in a PostgreSQL cluster.\n This process includes the following actions: * pick a target replica to become the new primary * delete the current primary deployment to avoid user requests from going to multiple primary databases (split brain) * promote the targeted replica using pg_ctl promote, this will cause PostgreSQL to go into read-write mode * re-label the targeted replica to use the primary labels, this will match the primary service selector and cause new requests to the primary to be routed to the new primary (targeted replica)\n The command works like this:\n pgo failover mycluster --query   That command will show you a list of replica targets you can choose to failover to. You will select one of those for the following command:\n pgo failover mycluster --target=mycluster-abxq   There is a CRD called pgtask that will hold the failover request and also the status of that request. You can view the status by viewing it:\n kubectl get pgtasks mycluster-failover -o yaml   Once completed, you will see a new replica has been started to replace the promoted replica, this happens automatically due to the re-lable, the Deployment will recreate its pod because of this. The failover typically takes only a few seconds, however, the creation of the replacement replica can take longer depending on how much data is being replicated.\n    "
},
{
	"uri": "/",
	"title": "Crunchy Data PostgreSQL Operator",
	"tags": [],
	"description": "",
	"content": "  PostgreSQL Operator v2.6, 2018-04-25\n The postgres-operator is a controller that runs within a Kubernetes cluster that provides a means to deploy and manage PostgreSQL clusters.\n Use the postgres-operator to -\n   deploy PostgreSQL containers including streaming replication clusters\n  scale up PostgreSQL clusters with extra replicas\n  add pgpool and metrics sidecars to PostgreSQL clusters\n  apply SQL policies to PostgreSQL clusters\n  assign metadata tags to PostgreSQL clusters\n  maintain PostgreSQL users and passwords\n  perform minor and major upgrades to PostgreSQL clusters\n  load simple CSV and JSON files into PostgreSQL clusters\n  perform database backups\n   Design The postgres-operator design incorporates the following concepts -\n   adds Custom Resource Definitions for PostgreSQL to Kubernetes\n  adds controller logic that watches events on PostgreSQL resources\n  provides a command line client (pgo) and REST API for interfacing with the postgres-operator\n  provides for very customized deployments including container resources, storage configurations, and PostgreSQL custom configurations\n   More design information is found on the How It Works page.\n   Requirements The postgres-operator runs on any Kubernetes and Openshift platform that supports Custom Resource Definitions.\n The Operator project builds and operates with the following containers -\n   PVC Listing Container\n  Remove Data Container\n  postgres-operator Container\n  apiserver Container\n  file load Container\n   This Operator is developed and tested on the following operating systems but is known to run on other operating systems -\n   CentOS 7\n  RHEL 7\n     Installation To build and deploy the Operator on your Kubernetes system, follow the instructions documented on the Installation page.\n Quickstart scripts are currently provided for GKE and OpenShift environments that can also be found on the link:/installation/Installation] page.\n If you\u0026#8217;re seeking to upgrade your existing Operator installation, please visit the Upgrading the Operator page.\n   Configuration The operator is template-driven; this makes it simple to configure both the client and the operator. The configuration options are documented on the Configuration page.\n   Getting Started postgres-operator commands are documented on the Getting Started page.\n   "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]